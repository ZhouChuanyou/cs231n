# cs231n Assignment 1 cww note

做cs231n的作业的一个心路历程，不会重复别人写好的材料，别人写的数学分析，代码，比我高到不知道哪里去了

We are [here](https://github.com/cww97/cs231n)

这也是一门经典的课程了，网上材料一堆，在自己很菜的情况下不上网查是不可能的，查了哪些引用了哪些我直接链接过去，这里记录自己的探索过程，并没有什么阅读价值~


## KNN

在cs231-lec2中讲解了这个很粗暴的算法：比较两张图片的每一个像素作为distance，对于每一个测试选取训练集中与之dist最小的。

比较有意思的是里面计算距离矩阵的三种方法, `two_loops`, `one_loop`, `no_loop`, 完全平方公式还有python中的broadcast比较有意思，还是能学到东西的。

最后的交叉验证，之前在博哥的ML课上做过。所以就skip了

赶紧看模态融合啊，先鸽了，明天之后再写这个。(2019.12.16)


## SVM

梯度怎么求，这谁记得住啊？救

[回忆SVM的loss](https://zhuanlan.zhihu.com/p/21478575)

`scores = X[i].dot(W)`

注意到这里是`X[i]` 在前，`W`在后，X的shape是(500, 3073)，那么`X[i]`便是一个长度为3073的行向量,而W是(3073, 10)。

第i张图片在第j个分类的得分只与W的第j列有关，所以当`margin > 0`即有loss的时候，需要调整W使错误分类的得分变低，使正确分类的得分变高。

从[这里](https://zhuanlan.zhihu.com/p/21478575)的分析可以看出，对于W的这一列，直接求偏导，对第j列就是`X[i]`，对第`y[i]`（正确标签）列就是`-X[i]`（求偏导就当求导，多看两遍，我数学这么烂都看懂了）

不过对于最后的正则化项为什么要这么写我还是很疑惑的`dW += 2 * reg * W `老陈问号.jpg

没有循环的矩阵运算有点骚，看湿了。

写完两个loss还有train和predict的代码要写，其实比前面的好写多了，因为不用算梯度

learning_rates = [1e-7, 5e-5]  # 第二个lr要加一个小数点才能收敛，不然就炸了

调了半天以为前面code写错了，学习率过大导致每一步都跑过了，然后loss爆炸，再加一个小数点就好了，感觉这个lr写这是故意让人理解的

## softmax

主要看了[这里](https://zhuanlan.zhihu.com/p/31562236)和原来的课件，卡的地方还是求梯度的过程

val过程其实和前面很像，我好菜啊

## two_layer_net

两层的全联通网络，激活函数在第一层`maximum(H, 0)`，scores很好算，loss可以直接抄softmax的，算梯度数学太差逃了，抄[这里](https://github.com/MahanFathi/CS231/blob/master/assignment1/cs231n/classifiers/neural_net.py)的

另外需要完成train和predict两个函数，和linear_classifier差不多，复制过来该一些变量就行（这个不涉及数学，一点都不慌）

toy_data和toy_model真的是调试神器，而且写一小段就有一个check点不要太舒服，那么问题来了，自己写这种东西的时候check数据哪来呢


“Tweaking hyperparameters by hand can be fun”那么现在调参都一天试一组参数？这个for循环写的感觉有点奢侈啊，而且超参啥的感觉都是xjb写，调参玄学？遇到再说吧，感觉现在自己的功力还没到调参这一步就死掉了


## features

前面是直接把图片像素直接扔进模型里，这一个part是首先提取出图片的feature然后把feature作为model的输入训练

要写的代码就是直接的调参，复制过来改一下参数，feature计算比像素点小很多，自然速度快很多

注意到NN的最好的一组超参达到了60%的准确率，题面描述的最佳效果，头秃了一晚上这个时候感觉最爽

## Summary

第一次作业使用numpy实现了KNN, SVM, softmax, 两层全联通网络，还有设计图像feature然后调参训练

说是实现其实抬举自己了，本来的代码写的非常优雅，读起来赏心悦目。只要在挖空的地方填上loss, train, predict之类的就好

数学菜的扣脚，各种梯度不会算，往回翻了课件最后还是跌跌撞撞网上各种抄才写完模型里的代码，我靠numpy真骚

不过正儿八经上战场应该还是`cuda()`吧，没能完全靠自己写完这些code，至少也都认真读了一遍，调参过程倒是熟练

多认识了不少超参，实实在在理解了这几个模型，至少以后别人问到自己不会一问三不知了，ok，下一课

学校要三月开学了，被困在农村里，这一个晚上的效率感觉是过去一周，不，一个月的总和，希望状态能保持

希望早日能见到夏天的阳光，夏天的YOUNG : )
